import requests
import xml.etree.ElementTree as ET
from urllib.parse import urljoin
from typing import List
import json
import csv
import os


class SitemapService:
    """
    Service l·∫•y to√†n b·ªô URL t·ª´ website th√¥ng qua sitemap
    - T·ª± ph√°t hi·ªán sitemap t·ª´ robots.txt
    - H·ªó tr·ª£ sitemap index (ƒë·ªá quy)
    - Xu·∫•t JSON / CSV
    """

    def __init__(self, site_url: str, timeout: int = 15):
        self.site_url = site_url.rstrip("/")
        self.timeout = timeout
        self.sitemap_url: str | None = None
        self.urls: List[str] = []

    # =========================
    # 1. PH√ÅT HI·ªÜN SITEMAP
    # =========================
    def detect_sitemap(self) -> str | None:
        robots_url = urljoin(self.site_url, "/robots.txt")

        try:
            r = requests.get(robots_url, timeout=self.timeout)
            if r.status_code != 200:
                return None

            for line in r.text.splitlines():
                if line.lower().startswith("sitemap:"):
                    self.sitemap_url = line.split(":", 1)[1].strip()
                    return self.sitemap_url

        except Exception as e:
            print("‚ùå L·ªói khi ƒë·ªçc robots.txt:", e)

        return None

    # =========================
    # 2. PARSE SITEMAP ƒê·ªÜ QUY
    # =========================
    def _parse_sitemap_recursive(self, sitemap_url: str) -> List[str]:
        r = requests.get(sitemap_url, timeout=self.timeout)
        r.raise_for_status()

        root = ET.fromstring(r.text)
        ns = {"ns": "http://www.sitemaps.org/schemas/sitemap/0.9"}
        urls: List[str] = []

        # ‚úÖ Sitemap index (cha)
        if root.tag.endswith("sitemapindex"):
            for loc in root.findall(".//ns:loc", ns):
                child_sitemap = loc.text.strip()
                print("üìÇ ƒê·ªçc sitemap con:", child_sitemap)
                urls.extend(self._parse_sitemap_recursive(child_sitemap))

        # ‚úÖ Sitemap ch·ª©a URL th·∫≠t
        elif root.tag.endswith("urlset"):
            for loc in root.findall(".//ns:loc", ns):
                urls.append(loc.text.strip())

        return urls

    # =========================
    # 3. LOAD TO√ÄN B·ªò URL
    # =========================
    def load_all_urls(self) -> List[str]:
        if not self.sitemap_url:
            self.detect_sitemap()

        if not self.sitemap_url:
            raise ValueError("‚ùå Kh√¥ng t√¨m th·∫•y sitemap trong robots.txt")

        print("‚úÖ T√¨m th·∫•y sitemap:", self.sitemap_url)

        self.urls = self._parse_sitemap_recursive(self.sitemap_url)
        self.urls = self._filter_valid_urls(self.urls)

        return self.urls

    # =========================
    # 4. L·ªåC URL S·∫†CH
    # =========================
    def _filter_valid_urls(self, urls: List[str]) -> List[str]:
        blacklist_ext = (".jpg", ".png", ".js", ".css", ".pdf", ".zip", ".xml")
        return [u for u in urls if not u.lower().endswith(blacklist_ext)]

    # =========================
    # 5. EXPORT JSON
    # =========================
    def export_json(self, file_path: str):
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(self.urls, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ ƒê√£ l∆∞u JSON: {file_path}")

    # =========================
    # 6. EXPORT CSV
    # =========================
    def export_csv(self, file_path: str):
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        with open(file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["url"])
            for u in self.urls:
                writer.writerow([u])

        print(f"‚úÖ ƒê√£ l∆∞u CSV: {file_path}")

service = SitemapService("https://www.tnc.com.vn")

urls = service.load_all_urls()

print("T·ªïng URL:", len(urls))
for u in urls[:10]:
    print(u)

service.export_json("output/tnc_urls.json")
service.export_csv("output/tnc_urls.csv")